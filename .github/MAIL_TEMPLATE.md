
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ÊØèÊó•ArXivËÆ∫ÊñáÊõ¥Êñ∞</title>
</head>
<body style="
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    line-height: 1.6;
    color: #333;
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
    background-color: #ffffff;
">
    <div style="
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 30px;
        border-radius: 10px;
        text-align: center;
        margin-bottom: 30px;
    ">
        <h1 style="margin: 0; font-size: 28px;">üìß ÊØèÊó•ArXivËÆ∫ÊñáÊõ¥Êñ∞</h1>
        <p style="margin: 10px 0 0 0; font-size: 16px; opacity: 0.9;">
            ÊúÄÊñ∞ÁöÑÂ≠¶ÊúØËÆ∫ÊñáÔºåÁõ¥ËææÊÇ®ÁöÑÈÇÆÁÆ±
        </p>
    </div>
    
    <div style="
        background-color: #f8f9fa;
        padding: 20px;
        border-radius: 8px;
        margin-bottom: 25px;
        border-left: 4px solid #007bff;
    ">
        <p style="margin: 0; color: #495057;">
            <strong>üí° ÊèêÁ§∫Ôºö</strong>
            ËØ∑Êü•Áúã <a href="https://github.com/LbsTempest/Daily-ArXiv-Subscription" style="color: #007bff; text-decoration: none;">GithubÈ°µÈù¢</a> 
            Ëé∑ÂæóÊõ¥Â•ΩÁöÑÈòÖËØª‰ΩìÈ™åÂíåÊõ¥Â§öËÆ∫Êñá„ÄÇ
        </p>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Speech Synthesis</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2509.24629v2" style="color: #0066cc; text-decoration: none;"><strong>Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.06560v1" style="color: #0066cc; text-decoration: none;"><strong>Lightweight Resolution-Aware Audio Deepfake Detection via Cross-Scale Attention and Consistency Learning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-10</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.03727v1" style="color: #0066cc; text-decoration: none;"><strong>Stuttering-Aware Automatic Speech Recognition for Indonesian Language</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-07</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Preprint</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.03632v1" style="color: #0066cc; text-decoration: none;"><strong>ReStyle-TTS: Relative and Continuous Style Control for Zero-Shot Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-07</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.03403v1" style="color: #0066cc; text-decoration: none;"><strong>Tigrinya Number Verbalization: Rules, Algorithm, and Implementation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-06</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.02914v1" style="color: #0066cc; text-decoration: none;"><strong>Vulnerabilities of Audio-Based Biometric Authentication Systems Against Deepfake Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-06</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.02444v1" style="color: #0066cc; text-decoration: none;"><strong>VocalBridge: Latent Diffusion-Bridge Purification for Defeating Perturbation-Based Voiceprint Defenses</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-05</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.01459v1" style="color: #0066cc; text-decoration: none;"><strong>OV-InstructTTS: Towards Open-Vocabulary Instruct Text-to-Speech</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-04</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.13251v3" style="color: #0066cc; text-decoration: none;"><strong>DisCo-Speech: Controllable Zero-Shot Speech Generation with A Disentangled Speech Codec</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-04</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Updated with 6,000 hours of additional training da...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.23300v1" style="color: #0066cc; text-decoration: none;"><strong>AI4Reading: Chinese Audiobook Interpretation System Based on Multi-Agent Collaboration</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">ACL 2025 demo</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.22491v1" style="color: #0066cc; text-decoration: none;"><strong>ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching and Hierarchical Text Representation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-27</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.21702v1" style="color: #0066cc; text-decoration: none;"><strong>Zero-Shot to Zero-Lies: Detecting Bengali Deepfake Audio through Transfer Learning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted for publication in 2025 28th Internationa...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2506.09349v4" style="color: #0066cc; text-decoration: none;"><strong>DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Work in progress</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.18699v1" style="color: #0066cc; text-decoration: none;"><strong>Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-21</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.14291v1" style="color: #0066cc; text-decoration: none;"><strong>GLM-TTS Technical Report</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">TTS</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.05554v1" style="color: #0066cc; text-decoration: none;"><strong>SPAM: Style Prompt Adherence Metric for Prompt-based TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-09</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.04656v1" style="color: #0066cc; text-decoration: none;"><strong>FlexiVoice: Enabling Flexible Style Control in Zero-Shot TTS with Natural Language Instructions</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-08</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2509.09631v3" style="color: #0066cc; text-decoration: none;"><strong>DiFlow-TTS: Compact and Low-Latency Zero-Shot Text-to-Speech with Factorized Discrete Flow Matching</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-07</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.03632v1" style="color: #0066cc; text-decoration: none;"><strong>ReStyle-TTS: Relative and Continuous Style Control for Zero-Shot Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-07</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.01903v1" style="color: #0066cc; text-decoration: none;"><strong>TT-FSI: Scalable Faithful Shapley Interactions via Tensor-Train</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-05</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.00935v1" style="color: #0066cc; text-decoration: none;"><strong>Improving Code-Switching Speech Recognition with TTS Data Augmentation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-02</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">This paper was accepted by APSIPA 2025</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.19433v1" style="color: #0066cc; text-decoration: none;"><strong>dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Project page: https://github.com/Alpha-VLLM/Lumina...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.18699v1" style="color: #0066cc; text-decoration: none;"><strong>Task Vector in TTS: Toward Emotionally Expressive Dialectal Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-21</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.17293v1" style="color: #0066cc; text-decoration: none;"><strong>Robust TTS Training via Self-Purifying Flow Matching for the WildSpoof 2026 TTS Track</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">2 pages, preprint, This work has been submitted to...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.14291v1" style="color: #0066cc; text-decoration: none;"><strong>GLM-TTS Technical Report</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.12297v1" style="color: #0066cc; text-decoration: none;"><strong>F5-TTS-RO: Extending F5-TTS to Romanian TTS via Lightweight Input Adaptation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-13</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted at The 20th International Conference on L...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.09504v1" style="color: #0066cc; text-decoration: none;"><strong>DMP-TTS: Disentangled multi-modal Prompting for Controllable Text-to-Speech with Chained Guidance</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-10</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.08006v1" style="color: #0066cc; text-decoration: none;"><strong>Beyond Unified Models: A Service-Oriented Approach to Low Latency, Context Aware Phonemization for Real Time TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-08</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.04720v1" style="color: #0066cc; text-decoration: none;"><strong>M3-TTS: Multi-modal DiT Alignment & Mel-latent for Zero-shot High-fidelity Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-04</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2026</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.04552v1" style="color: #0066cc; text-decoration: none;"><strong>RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-04</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2026. Copyright 2026 IEEE. Per...</span></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Audio Caption</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.02731v2" style="color: #0066cc; text-decoration: none;"><strong>Omni2Sound: Towards Unified Video-Text-to-Audio Generation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.04658v1" style="color: #0066cc; text-decoration: none;"><strong>LAMB: LLM-based Audio Captioning with Modality Gap Bridging via Cauchy-Schwarz Divergence</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-08</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 2 figures;</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.10403v1" style="color: #0066cc; text-decoration: none;"><strong>BRACE: A Benchmark for Robust Audio Caption Quality Evaluation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.10170v1" style="color: #0066cc; text-decoration: none;"><strong>Semantic-Aware Confidence Calibration for Automated Audio Captioning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 2 figures</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2511.13273v1" style="color: #0066cc; text-decoration: none;"><strong>Spatial Blind Spot: Auditory Motion Perception Deficits in Audio LLMs</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-11-17</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2508.03983v2" style="color: #0066cc; text-decoration: none;"><strong>MiDashengLM: Efficient Audio Understanding with General Audio Captions</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-11-13</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2507.18452v3" style="color: #0066cc; text-decoration: none;"><strong>DIFFA: Large Language Diffusion Models Can Listen and Understand</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-11-10</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted by AAAI 2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2511.01670v1" style="color: #0066cc; text-decoration: none;"><strong>SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-11-03</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">10 pages</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2510.14249v1" style="color: #0066cc; text-decoration: none;"><strong>Do Joint Language-Audio Embeddings Encode Perceptual Timbre Semantics?</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2510.12720v1" style="color: #0066cc; text-decoration: none;"><strong>Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">https://github.com/ddlBoJack/Omni-Captioner</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2510.12851v1" style="color: #0066cc; text-decoration: none;"><strong>Adaptive vector steering: A training-free, layer-wise intervention for hallucination mitigation in large audio and multimodal models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Note: This preprint is a version of the paper subm...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2510.11330v1" style="color: #0066cc; text-decoration: none;"><strong>Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-13</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages. Submitted to IEEE ICASSP 2026</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2510.04934v1" style="color: #0066cc; text-decoration: none;"><strong>AURA Score: A Metric For Holistic Audio Question Answering Evaluation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-06</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2510.03117v1" style="color: #0066cc; text-decoration: none;"><strong>Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-03</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2509.24635v1" style="color: #0066cc; text-decoration: none;"><strong>When Audio Generators Become Good Listeners: Generative Features for Understanding Tasks</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Speech language model</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.06972v1" style="color: #0066cc; text-decoration: none;"><strong>Categorize Early, Integrate Late: Divergent Processing Strategies in Automatic Speech Recognition</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">3 figures, 9 tables</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.06199v1" style="color: #0066cc; text-decoration: none;"><strong>FastSLM: Hierarchical Frame Q-Former for Effective Speech Modality Adaptation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-08</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.04638v1" style="color: #0066cc; text-decoration: none;"><strong>SpeechMedAssist: Efficiently and Effectively Adapting Speech Language Models for Medical Consultation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-08</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2601.01461v1" style="color: #0066cc; text-decoration: none;"><strong>Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2026-01-04</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 1 figure</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.14657v1" style="color: #0066cc; text-decoration: none;"><strong>Adapting Speech Language Model to Singing Voice Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted by NeurIPS 2025 workshop AI for Music</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.14234v1" style="color: #0066cc; text-decoration: none;"><strong>ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Project page: https://ai.stanford.edu/~juze/ViBES/</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2510.16841v2" style="color: #0066cc; text-decoration: none;"><strong>SAC: Neural Speech Codec with Semantic-Acoustic Dual-Stream Quantization</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2512.01865v1" style="color: #0066cc; text-decoration: none;"><strong>Cross-Lingual Interleaving for Speech Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-12-01</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2511.22687v1" style="color: #0066cc; text-decoration: none;"><strong>PURE Codec: Progressive Unfolding of Residual Entropy for Speech Codec Learning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-11-27</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted by ASRU2025</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2511.22229v1" style="color: #0066cc; text-decoration: none;"><strong>VSpeechLM: A Visual Speech Language Model for Visual Text-to-Speech Task</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-11-27</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">MM Asia 2025</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2508.08961v3" style="color: #0066cc; text-decoration: none;"><strong>DualSpeechLM: Towards Unified Speech Understanding and Generation via Dual Speech Token Modeling with Large Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-11-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted by AAAI 2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2509.04685v3" style="color: #0066cc; text-decoration: none;"><strong>Say More with Less: Variable-Frame-Rate Speech Tokenization via Adaptive Clustering and Implicit Duration Coding</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-11-13</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted to AAAI 2026. Project page: https://zheng...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2511.10262v1" style="color: #0066cc; text-decoration: none;"><strong>MTR-DuplexBench: Towards a Comprehensive Evaluation of Multi-Round Conversations for Full-Duplex Speech Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-11-13</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Work in progress</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2510.01157v2" style="color: #0066cc; text-decoration: none;"><strong>Backdoor Attacks Against Speech Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-11-13</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/abs/2508.01166v2" style="color: #0066cc; text-decoration: none;"><strong>Hearing More with Less: Multi-Modal Retrieval-and-Selection Augmented Conversational LLM-Based ASR</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-11-12</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">AAAI 2026</span></td></tr></table>
    </div>

    <div style="
        margin-top: 40px;
        padding: 20px;
        background-color: #e9ecef;
        border-radius: 8px;
        text-align: center;
        color: #6c757d;
        font-size: 14px;
    ">
        <p style="margin: 0;">
            Êú¨ÈÇÆ‰ª∂Áî± <strong>GitHub Actions</strong> Ëá™Âä®ÁîüÊàê<br>
            Â¶ÇÊúâÈóÆÈ¢òÔºåËØ∑ËÆøÈóÆÈ°πÁõÆ‰ªìÂ∫ìÊàñËÅîÁ≥ªÁÆ°ÁêÜÂëò
        </p>
    </div>
</body>
</html>
        