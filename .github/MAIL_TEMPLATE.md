
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¯æ—¥ArXivè®ºæ–‡æ›´æ–°</title>
</head>
<body style="
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    line-height: 1.6;
    color: #333;
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
    background-color: #ffffff;
">
    <div style="
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 30px;
        border-radius: 10px;
        text-align: center;
        margin-bottom: 30px;
    ">
        <h1 style="margin: 0; font-size: 28px;">ğŸ“§ æ¯æ—¥ArXivè®ºæ–‡æ›´æ–°</h1>
        <p style="margin: 10px 0 0 0; font-size: 16px; opacity: 0.9;">
            æœ€æ–°çš„å­¦æœ¯è®ºæ–‡ï¼Œç›´è¾¾æ‚¨çš„é‚®ç®±
        </p>
    </div>
    
    <div style="
        background-color: #f8f9fa;
        padding: 20px;
        border-radius: 8px;
        margin-bottom: 25px;
        border-left: 4px solid #007bff;
    ">
        <p style="margin: 0; color: #495057;">
            <strong>ğŸ’¡ æç¤ºï¼š</strong>
            è¯·æŸ¥çœ‹ <a href="https://github.com/LbsTempest/Daily-ArXiv-Subscription" style="color: #007bff; text-decoration: none;">Githubé¡µé¢</a> 
            è·å¾—æ›´å¥½çš„é˜…è¯»ä½“éªŒå’Œæ›´å¤šè®ºæ–‡ã€‚
        </p>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Speech Synthesis</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2204.02524v3" style="color: #0066cc; text-decoration: none;"><strong>Simple and Effective Unsupervised Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2022-04-21</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">preprint, equal contribution from first two author...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2312.05814v2" style="color: #0066cc; text-decoration: none;"><strong>Neural Speech Embeddings for Speech Synthesis Based on Deep Generative Networks</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-02-28</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">4 pages</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2010.02636v1" style="color: #0066cc; text-decoration: none;"><strong>Neural Speech Synthesis for Estonian</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2020-10-07</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">9 pages in Estonian</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2401.13891v1" style="color: #0066cc; text-decoration: none;"><strong>Text to speech synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-01-26</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2406.08989v2" style="color: #0066cc; text-decoration: none;"><strong>ToneUnit: A Speech Discretization Approach for Tonal Language Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-09-04</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2108.11436v1" style="color: #0066cc; text-decoration: none;"><strong>Integrated Speech and Gesture Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2021-08-27</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">9 pages, accepted at ICMI 2021</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2006.04136v1" style="color: #0066cc; text-decoration: none;"><strong>Analysis and Synthesis of Hypo and Hyperarticulated Speech</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2020-06-09</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2002.12756v2" style="color: #0066cc; text-decoration: none;"><strong>Speech Synthesis using EEG</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2020-05-05</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted for publication at IEEE ICASSP 2020</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2108.00500v1" style="color: #0066cc; text-decoration: none;"><strong>End to End Bangla Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2021-08-03</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2008.00620v2" style="color: #0066cc; text-decoration: none;"><strong>Audiovisual Speech Synthesis using Tacotron2</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2021-08-31</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">This work has been submitted to the 23rd ACM Inter...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/1709.07552v1" style="color: #0066cc; text-decoration: none;"><strong>Techniques and Challenges in Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2017-09-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">138 pages, 46 figures, Undergraduate Honours Thesi...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2105.04124v1" style="color: #0066cc; text-decoration: none;"><strong>MASS: Multi-task Anthropomorphic Speech Synthesis Framework</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2021-05-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2404.14700v4" style="color: #0066cc; text-decoration: none;"><strong>FlashSpeech: Efficient Zero-Shot Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-10-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Efficient zero-shot speech synthesis</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2209.06337v1" style="color: #0066cc; text-decoration: none;"><strong>Deep Speech Synthesis from Articulatory Representations</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2022-09-15</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2407.14006v1" style="color: #0066cc; text-decoration: none;"><strong>MSceneSpeech: A Multi-Scene Speech Dataset For Expressive Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-07-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted by INTERSPEECH 2024</span></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">TTS</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2110.07840v1" style="color: #0066cc; text-decoration: none;"><strong>ESPnet2-TTS: Extending the Edge of TTS Research</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2021-10-18</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP2022. Demo HP: https://espnet.g...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2010.13421v1" style="color: #0066cc; text-decoration: none;"><strong>TTS-by-TTS: TTS-driven Data Augmentation for Fast and High-Quality Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2020-10-27</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2021</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2305.12107v2" style="color: #0066cc; text-decoration: none;"><strong>EE-TTS: Emphatic Expressive TTS with Linguistic Information</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-05-27</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted by Interspeech 2023, fix some typos</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2409.09351v1" style="color: #0066cc; text-decoration: none;"><strong>E1 TTS: Simple and Fast Non-Autoregressive TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-09-17</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2406.18009v2" style="color: #0066cc; text-decoration: none;"><strong>E2 TTS: Embarrassingly Easy Fully Non-Autoregressive Zero-Shot TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-09-13</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted to SLT 2024. Added evaluation data, see h...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2309.03199v2" style="color: #0066cc; text-decoration: none;"><strong>Matcha-TTS: A fast TTS architecture with conditional flow matching</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-01-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 3 figures. Final version, accepted to IEE...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2507.21138v1" style="color: #0066cc; text-decoration: none;"><strong>TTS-1 Technical Report</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-07-30</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">20 pages, 10 figures. For associated modeling and ...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2508.11326v1" style="color: #0066cc; text-decoration: none;"><strong>MoE-TTS: Enhancing Out-of-Domain Text Understanding for Description-based TTS via Mixture-of-Experts</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-18</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2510.05758v1" style="color: #0066cc; text-decoration: none;"><strong>EMORL-TTS: Reinforcement Learning for Fine-Grained Emotion Control in LLM-based TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-08</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Under review for ICASSP 2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2108.10447v1" style="color: #0066cc; text-decoration: none;"><strong>One TTS Alignment To Rule Them All</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2021-08-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2401.13851v2" style="color: #0066cc; text-decoration: none;"><strong>Scaling NVIDIA's Multi-speaker Multi-lingual TTS Systems with Zero-Shot TTS to Indic Languages</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-01-30</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Presentation accepted at ICASSP 2024</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2104.01409v1" style="color: #0066cc; text-decoration: none;"><strong>Diff-TTS: A Denoising Diffusion Model for Text-to-Speech</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2021-04-06</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to INTERSPEECH 2021</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2308.12614v2" style="color: #0066cc; text-decoration: none;"><strong>Obstruction characterization of co-TT graphs</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-11-15</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">arXiv admin note: substantial text overlap with ar...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/1705.10152v1" style="color: #0066cc; text-decoration: none;"><strong>Tangent Cones to TT Varieties</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2017-05-30</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2501.09104v2" style="color: #0066cc; text-decoration: none;"><strong>A Non-autoregressive Model for Joint STT and TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-01-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 3 figures, 3 tables</span></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Audio Caption</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2309.07372v1" style="color: #0066cc; text-decoration: none;"><strong>Training Audio Captioning Models without Audio</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2023-09-15</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/1910.09387v1" style="color: #0066cc; text-decoration: none;"><strong>Clotho: An Audio Captioning Dataset</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2019-10-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2208.06127v1" style="color: #0066cc; text-decoration: none;"><strong>An investigation on selecting audio pre-trained models for audio captioning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2022-08-15</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 7 figures</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2309.08141v1" style="color: #0066cc; text-decoration: none;"><strong>Audio Difference Learning for Audio Captioning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2023-09-18</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">submitted to ICASSP2024</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2204.08409v1" style="color: #0066cc; text-decoration: none;"><strong>Caption Feature Space Regularization for Audio Captioning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2022-04-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2107.09817v1" style="color: #0066cc; text-decoration: none;"><strong>Audio Captioning Transformer</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2021-07-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 1 figure</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2309.09836v2" style="color: #0066cc; text-decoration: none;"><strong>RECAP: Retrieval-Augmented Audio Captioning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-06-07</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">ICASSP 2024. Code and data: https://github.com/Sre...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2411.00321v2" style="color: #0066cc; text-decoration: none;"><strong>MACE: Leveraging Audio for Evaluating Audio Captioning Systems</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-11-06</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2311.08396v1" style="color: #0066cc; text-decoration: none;"><strong>Zero-shot audio captioning with audio-language model guidance and audio context keywords</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2023-11-15</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">NeurIPS 2023 - Machine Learning for Audio Workshop...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2406.15487v2" style="color: #0066cc; text-decoration: none;"><strong>Improving Text-To-Audio Models with Synthetic Captions</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-07-10</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2210.16428v3" style="color: #0066cc; text-decoration: none;"><strong>Visually-Aware Audio Captioning With Adaptive Audio-Visual Attention</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2023-05-30</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">INTERSPEECH 2023</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2012.07331v1" style="color: #0066cc; text-decoration: none;"><strong>Audio Captioning using Pre-Trained Large-Scale Language Model Guided by Audio-based Similar Caption Retrieval</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2020-12-15</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2021</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2309.03326v1" style="color: #0066cc; text-decoration: none;"><strong>Detecting False Alarms and Misses in Audio Captions</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2023-09-08</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2509.14659v1" style="color: #0066cc; text-decoration: none;"><strong>Aligning Audio Captions with Human Preferences</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2026</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/1907.09238v1" style="color: #0066cc; text-decoration: none;"><strong>Crowdsourcing a Dataset of Audio Captions</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2019-07-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Speech language model</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2306.02982v2" style="color: #0066cc; text-decoration: none;"><strong>PolyVoice: Language Models for Speech to Speech Translation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2023-06-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/1809.06800v1" style="color: #0066cc; text-decoration: none;"><strong>Visual Speech Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2018-09-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Extended abstract based on Decoding Visemes: impro...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2308.16692v2" style="color: #0066cc; text-decoration: none;"><strong>SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-01-24</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted by ICLR 2024. Project page is at https://...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2406.11037v1" style="color: #0066cc; text-decoration: none;"><strong>NAST: Noise Aware Speech Tokenization for Speech Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-06-18</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted at Interspeech 2024</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2305.14838v2" style="color: #0066cc; text-decoration: none;"><strong>ComSL: A Composite Speech-Language Model for End-to-End Speech-to-Text Translation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2023-10-17</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">NeurIPS 2023, Poster</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2403.12408v1" style="color: #0066cc; text-decoration: none;"><strong>MSLM-S2ST: A Multitask Speech Language Model for Textless Speech-to-Speech Translation with Speaker Style Preservation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-03-20</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2509.12171v2" style="color: #0066cc; text-decoration: none;"><strong>Preservation of Language Understanding Capabilities in Speech-aware Large Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-17</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 1 figure; benchmark code available at htt...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2405.20410v1" style="color: #0066cc; text-decoration: none;"><strong>SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-06-03</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2308.15930v3" style="color: #0066cc; text-decoration: none;"><strong>LLaSM: Large Language and Speech Model</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2023-09-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2409.03701v2" style="color: #0066cc; text-decoration: none;"><strong>LAST: Language Model Aware Speech Tokenization</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-09-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2408.13040v1" style="color: #0066cc; text-decoration: none;"><strong>SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-08-26</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Published in IEEE/ACM Transactions on Audio, Speec...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2506.17611v1" style="color: #0066cc; text-decoration: none;"><strong>OpusLM: A Family of Open Unified Speech Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-06-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2406.18871v1" style="color: #0066cc; text-decoration: none;"><strong>DeSTA: Enhancing Speech Language Models through Descriptive Speech-Text Alignment</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2024-06-28</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted to Interspeech 2024</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2402.12208v4" style="color: #0066cc; text-decoration: none;"><strong>Language-Codec: Bridging Discrete Codec Representations and Speech Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-06-05</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">ACL 2025 Main</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="https://arxiv.org/pdf/2310.08715v1" style="color: #0066cc; text-decoration: none;"><strong>Toward Joint Language Modeling for Speech Units and Text</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2023-10-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">EMNLP findings 2023</span></td></tr></table>
    </div>

    <div style="
        margin-top: 40px;
        padding: 20px;
        background-color: #e9ecef;
        border-radius: 8px;
        text-align: center;
        color: #6c757d;
        font-size: 14px;
    ">
        <p style="margin: 0;">
            æœ¬é‚®ä»¶ç”± <strong>GitHub Actions</strong> è‡ªåŠ¨ç”Ÿæˆ<br>
            å¦‚æœ‰é—®é¢˜ï¼Œè¯·è®¿é—®é¡¹ç›®ä»“åº“æˆ–è”ç³»ç®¡ç†å‘˜
        </p>
    </div>
</body>
</html>
        