
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¯æ—¥ArXivè®ºæ–‡æ›´æ–°</title>
</head>
<body style="
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    line-height: 1.6;
    color: #333;
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
    background-color: #ffffff;
">
    <div style="
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 30px;
        border-radius: 10px;
        text-align: center;
        margin-bottom: 30px;
    ">
        <h1 style="margin: 0; font-size: 28px;">ğŸ“§ æ¯æ—¥ArXivè®ºæ–‡æ›´æ–°</h1>
        <p style="margin: 10px 0 0 0; font-size: 16px; opacity: 0.9;">
            æœ€æ–°çš„å­¦æœ¯è®ºæ–‡ï¼Œç›´è¾¾æ‚¨çš„é‚®ç®±
        </p>
    </div>
    
    <div style="
        background-color: #f8f9fa;
        padding: 20px;
        border-radius: 8px;
        margin-bottom: 25px;
        border-left: 4px solid #007bff;
    ">
        <p style="margin: 0; color: #495057;">
            <strong>ğŸ’¡ æç¤ºï¼š</strong>
            è¯·æŸ¥çœ‹ <a href="https://github.com/LbsTempest/Daily-ArXiv-Subscription" style="color: #007bff; text-decoration: none;">Githubé¡µé¢</a> 
            è·å¾—æ›´å¥½çš„é˜…è¯»ä½“éªŒå’Œæ›´å¤šè®ºæ–‡ã€‚
        </p>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Speech Synthesis</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.25131v1" style="color: #0066cc; text-decoration: none;"><strong>MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Code is available at https://github.com/dvlab-rese...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.24650v1" style="color: #0066cc; text-decoration: none;"><strong>VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Technical Report</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.24629v1" style="color: #0066cc; text-decoration: none;"><strong>Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.11425v2" style="color: #0066cc; text-decoration: none;"><strong>FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.24391v1" style="color: #0066cc; text-decoration: none;"><strong>UniFlow-Audio: Unified Flow Matching for Audio Generation from Omni-Modalities</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Project page: https://wsntxxn.github.io/uniflow_au...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.17052v2" style="color: #0066cc; text-decoration: none;"><strong>Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 1 figures</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.23618v1" style="color: #0066cc; text-decoration: none;"><strong>Generalizable Speech Deepfake Detection via Information Bottleneck Enhanced Adversarial Alignment</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-28</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2506.02863v2" style="color: #0066cc; text-decoration: none;"><strong>CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-26</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.22148v1" style="color: #0066cc; text-decoration: none;"><strong>Speaker Anonymisation for Speech-based Suicide Risk Detection</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-26</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.22062v1" style="color: #0066cc; text-decoration: none;"><strong>Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-26</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">conference paper about TTS</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.10412v2" style="color: #0066cc; text-decoration: none;"><strong>Facilitating Personalized TTS for Dysarthric Speakers Using Knowledge Anchoring and Curriculum Learning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Interspeech 2025</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2505.10599v2" style="color: #0066cc; text-decoration: none;"><strong>UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Under review</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.20485v1" style="color: #0066cc; text-decoration: none;"><strong>Objective Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional Prediction of Discrete Tokens</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-24</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Under review for IEEE OJSP</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.19883v1" style="color: #0066cc; text-decoration: none;"><strong>CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-24</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">13 pages, 5 figures, 5 tables</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.19812v1" style="color: #0066cc; text-decoration: none;"><strong>Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-24</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">6 pages of main text, 1 page of references, 2 figu...</span></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">TTS</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.24650v1" style="color: #0066cc; text-decoration: none;"><strong>VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Technical Report</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.20802v2" style="color: #0066cc; text-decoration: none;"><strong>SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-26</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2026</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.21718v1" style="color: #0066cc; text-decoration: none;"><strong>Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-26</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.10412v2" style="color: #0066cc; text-decoration: none;"><strong>Facilitating Personalized TTS for Dysarthric Speakers Using Knowledge Anchoring and Curriculum Learning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Interspeech 2025</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.22727v1" style="color: #0066cc; text-decoration: none;"><strong>DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 2 figures</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.19852v1" style="color: #0066cc; text-decoration: none;"><strong>Eliminating stability hallucinations in llm-based tts models via attention guidance</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-24</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, submitted to ICASSP2026</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.19001v1" style="color: #0066cc; text-decoration: none;"><strong>HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for Instruction-based TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 2 figures, submitted to ICASSP2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.18569v1" style="color: #0066cc; text-decoration: none;"><strong>Explore the Reinforcement Learning for the LLM based ASR and TTS system</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.18531v1" style="color: #0066cc; text-decoration: none;"><strong>No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">submitted to ICASSP 2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.17021v1" style="color: #0066cc; text-decoration: none;"><strong>Bridging the gap between training and inference in LM-based TTS models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-21</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 4 figures</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.09748v1" style="color: #0066cc; text-decoration: none;"><strong>DiTReducio: A Training-Free Acceleration for DiT-Based TTS via Progressive Calibration</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.05863v1" style="color: #0066cc; text-decoration: none;"><strong>LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-06</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.15442v3" style="color: #0066cc; text-decoration: none;"><strong>Mitigating Hallucinations in LM-Based TTS Models via Distribution Alignment Using GFlowNets</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-05</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted to EMNLP 2025 Main Conference (Oral)</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2506.23367v2" style="color: #0066cc; text-decoration: none;"><strong>You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-03</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted to ISCA Speech Synthesis Workshop, 2025, ...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.20513v1" style="color: #0066cc; text-decoration: none;"><strong>MoTAS: MoE-Guided Feature Selection from TTS-Augmented Speech for Enhanced Multimodal Alzheimer's Early Screening</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-28</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Audio Caption</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.24635v1" style="color: #0066cc; text-decoration: none;"><strong>When Audio Generators Become Good Listeners: Generative Features for Understanding Tasks</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2506.02863v2" style="color: #0066cc; text-decoration: none;"><strong>CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-26</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.01659v2" style="color: #0066cc; text-decoration: none;"><strong>From Contrast to Commonality: Audio Commonality Captioning for Enhanced Audio-Text Cross-modal Understanding in Multimodal LLMs</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.17765v1" style="color: #0066cc; text-decoration: none;"><strong>Qwen3-Omni Technical Report</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">https://github.com/QwenLM/Qwen3-Omni</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2502.15178v3" style="color: #0066cc; text-decoration: none;"><strong>Enhancing Speech Large Language Models with Prompt-Aware Mixture of Audio Encoders</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">16 pages,5 figures, 13 tables, to be published in ...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.15680v1" style="color: #0066cc; text-decoration: none;"><strong>Mamba-2 audio captioning: design space exploration and analysis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to the 2026 IEEE International Conferenc...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.14659v1" style="color: #0066cc; text-decoration: none;"><strong>Aligning Audio Captions with Human Preferences</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-18</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.12591v1" style="color: #0066cc; text-decoration: none;"><strong>MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted in The 26th International Conference on W...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.19514v1" style="color: #0066cc; text-decoration: none;"><strong>MQAD: A Large-Scale Question Answering Dataset for Training Music Large Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-27</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2406.05914v3" style="color: #0066cc; text-decoration: none;"><strong>Soundscape Captioning using Sound Affective Quality Network and Large Language Model</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">IEEE Transactions on Multimedia, Code:
  https://g...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2507.18452v2" style="color: #0066cc; text-decoration: none;"><strong>DIFFA: Large Language Diffusion Models Can Listen and Understand</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-21</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2409.12962v2" style="color: #0066cc; text-decoration: none;"><strong>CLAIR-A: Leveraging Large Language Models to Judge Audio Captions</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted to ASRU 2025; Code is publicly available ...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.07829v1" style="color: #0066cc; text-decoration: none;"><strong>Auditory Intelligence: Understanding the World Through Sound</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Position paper without experimental/quantitative v...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.03983v1" style="color: #0066cc; text-decoration: none;"><strong>MiDashengLM: Efficient Audio Understanding with General Audio Captions</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-06</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2507.18750v1" style="color: #0066cc; text-decoration: none;"><strong>CatchPhrase: EXPrompt-Guided Encoder Adaptation for Audio-to-Image Generation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-07-24</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Speech language model</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.11425v2" style="color: #0066cc; text-decoration: none;"><strong>FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2410.15017v2" style="color: #0066cc; text-decoration: none;"><strong>DM-Codec: Distilling Multimodal Representations for Speech Tokenization</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted at EMNLP 2025</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.21144v1" style="color: #0066cc; text-decoration: none;"><strong>UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.15362v2" style="color: #0066cc; text-decoration: none;"><strong>Speech Language Models for Under-Represented Languages: Insights from Wolof</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.18570v1" style="color: #0066cc; text-decoration: none;"><strong>HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages; submitted to ICASSP 2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.15655v1" style="color: #0066cc; text-decoration: none;"><strong>Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">EMNLP 2025 Main Conference (Oral)</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.14882v1" style="color: #0066cc; text-decoration: none;"><strong>Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-18</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 1 figures</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.13785v1" style="color: #0066cc; text-decoration: none;"><strong>Summary on The Multilingual Conversational Speech Language Model Challenge: Datasets, Tasks, Baselines, and Methods</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-17</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.04685v1" style="color: #0066cc; text-decoration: none;"><strong>Say More with Less: Variable-Frame-Rate Speech Tokenization via Adaptive Clustering and Implicit Duration Coding</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-04</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.05359v1" style="color: #0066cc; text-decoration: none;"><strong>An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-03</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Published in International Conference on Text, Spe...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.09600v2" style="color: #0066cc; text-decoration: none;"><strong>OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via Understanding-Driven Spoken Dialogue</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-03</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.00503v1" style="color: #0066cc; text-decoration: none;"><strong>Entropy-based Coarse and Compressed Semantic Speech Representation Learning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-30</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.20660v1" style="color: #0066cc; text-decoration: none;"><strong>CodecBench: A Comprehensive Benchmark for Acoustic and Semantic Evaluation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-28</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2505.01263v2" style="color: #0066cc; text-decoration: none;"><strong>FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.16790v1" style="color: #0066cc; text-decoration: none;"><strong>TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr></table>
    </div>

    <div style="
        margin-top: 40px;
        padding: 20px;
        background-color: #e9ecef;
        border-radius: 8px;
        text-align: center;
        color: #6c757d;
        font-size: 14px;
    ">
        <p style="margin: 0;">
            æœ¬é‚®ä»¶ç”± <strong>GitHub Actions</strong> è‡ªåŠ¨ç”Ÿæˆ<br>
            å¦‚æœ‰é—®é¢˜ï¼Œè¯·è®¿é—®é¡¹ç›®ä»“åº“æˆ–è”ç³»ç®¡ç†å‘˜
        </p>
    </div>
</body>
</html>
        