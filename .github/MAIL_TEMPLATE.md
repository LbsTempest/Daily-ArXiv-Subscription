
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¯æ—¥ArXivè®ºæ–‡æ›´æ–°</title>
</head>
<body style="
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    line-height: 1.6;
    color: #333;
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
    background-color: #ffffff;
">
    <div style="
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 30px;
        border-radius: 10px;
        text-align: center;
        margin-bottom: 30px;
    ">
        <h1 style="margin: 0; font-size: 28px;">ğŸ“§ æ¯æ—¥ArXivè®ºæ–‡æ›´æ–°</h1>
        <p style="margin: 10px 0 0 0; font-size: 16px; opacity: 0.9;">
            æœ€æ–°çš„å­¦æœ¯è®ºæ–‡ï¼Œç›´è¾¾æ‚¨çš„é‚®ç®±
        </p>
    </div>
    
    <div style="
        background-color: #f8f9fa;
        padding: 20px;
        border-radius: 8px;
        margin-bottom: 25px;
        border-left: 4px solid #007bff;
    ">
        <p style="margin: 0; color: #495057;">
            <strong>ğŸ’¡ æç¤ºï¼š</strong>
            è¯·æŸ¥çœ‹ <a href="https://github.com/LbsTempest/Daily-ArXiv-Subscription" style="color: #007bff; text-decoration: none;">Githubé¡µé¢</a> 
            è·å¾—æ›´å¥½çš„é˜…è¯»ä½“éªŒå’Œæ›´å¤šè®ºæ–‡ã€‚
        </p>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Speech Synthesis</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2505.19687v2" style="color: #0066cc; text-decoration: none;"><strong>DiEmo-TTS: Disentangled Emotion Representations via Self-Supervised Distillation for Cross-Speaker Emotion Transfer in Text-to-Speech</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-17</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Proceedings of Interspeech 2025</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.14628v1" style="color: #0066cc; text-decoration: none;"><strong>RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.13632v1" style="color: #0066cc; text-decoration: none;"><strong>Closing the Gap Between Text and Speech Understanding in LLMs</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-15</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.12995v1" style="color: #0066cc; text-decoration: none;"><strong>Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.10774v2" style="color: #0066cc; text-decoration: none;"><strong>ParsVoice: A Large-Scale Multi-Speaker Persian Speech Corpus for Text-to-Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.11646v1" style="color: #0066cc; text-decoration: none;"><strong>BridgeCode: A Dual Speech Representation Paradigm for Autoregressive Zero-Shot Text-to-Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-13</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.11124v1" style="color: #0066cc; text-decoration: none;"><strong>Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-13</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to Expert Systems with Applications,11 p...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.18470v2" style="color: #0066cc; text-decoration: none;"><strong>Discrete-Time Diffusion-Like Models for Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-13</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.05096v2" style="color: #0066cc; text-decoration: none;"><strong>Paper2Video: Automatic Video Generation from Scientific Papers</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-09</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Project Page: https://showlab.github.io/Paper2Vide...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.07979v1" style="color: #0066cc; text-decoration: none;"><strong>IntMeanFlow: Few-step Speech Generation with Integral Velocity Distillation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-09</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.07096v1" style="color: #0066cc; text-decoration: none;"><strong>Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-08</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.06706v1" style="color: #0066cc; text-decoration: none;"><strong>XLSR-Kanformer: A KAN-Intergrated model for Synthetic Speech Detection</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-08</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted to 2025 IEEE International Conference on ...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.05984v1" style="color: #0066cc; text-decoration: none;"><strong>ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency Tuning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-07</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted for publication by Proceedings of the 202...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.05696v1" style="color: #0066cc; text-decoration: none;"><strong>Sparse deepfake detection promotes better disentanglement</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-07</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2504.20629v2" style="color: #0066cc; text-decoration: none;"><strong>AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-03</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">ACM Multimedia 2025</span></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">TTS</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.13293v1" style="color: #0066cc; text-decoration: none;"><strong>Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive TTS Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-15</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.12995v1" style="color: #0066cc; text-decoration: none;"><strong>Continuous-Token Diffusion for Speaker-Referenced TTS in Multimodal LLMs</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.11124v1" style="color: #0066cc; text-decoration: none;"><strong>Perturbation Self-Supervised Representations for Cross-Lingual Emotion TTS: Stage-Wise Modeling of Emotion and Speaker</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-13</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to Expert Systems with Applications,11 p...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.05758v1" style="color: #0066cc; text-decoration: none;"><strong>EMORL-TTS: Reinforcement Learning for Fine-Grained Emotion Control in LLM-based TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-07</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Under review for ICASSP 2026</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.04738v1" style="color: #0066cc; text-decoration: none;"><strong>Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-06</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.04593v1" style="color: #0066cc; text-decoration: none;"><strong>UniVoice: Unifying Autoregressive ASR and Flow-Matching based TTS with Large Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-06</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.24650v1" style="color: #0066cc; text-decoration: none;"><strong>VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Technical Report</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.20802v2" style="color: #0066cc; text-decoration: none;"><strong>SPADE: Structured Pruning and Adaptive Distillation for Efficient LLM-TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-26</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2026</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.21718v1" style="color: #0066cc; text-decoration: none;"><strong>Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-26</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.10412v2" style="color: #0066cc; text-decoration: none;"><strong>Facilitating Personalized TTS for Dysarthric Speakers Using Knowledge Anchoring and Curriculum Learning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Interspeech 2025</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.22727v1" style="color: #0066cc; text-decoration: none;"><strong>DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 2 figures</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.19852v1" style="color: #0066cc; text-decoration: none;"><strong>Eliminating stability hallucinations in llm-based tts models via attention guidance</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-24</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, submitted to ICASSP2026</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.19001v1" style="color: #0066cc; text-decoration: none;"><strong>HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for Instruction-based TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 2 figures, submitted to ICASSP2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.18569v1" style="color: #0066cc; text-decoration: none;"><strong>Explore the Reinforcement Learning for the LLM based ASR and TTS system</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.18531v1" style="color: #0066cc; text-decoration: none;"><strong>No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">submitted to ICASSP 2026</span></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Audio Caption</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.14249v1" style="color: #0066cc; text-decoration: none;"><strong>Do Joint Language-Audio Embeddings Encode Perceptual Timbre Semantics?</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.12720v1" style="color: #0066cc; text-decoration: none;"><strong>Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">https://github.com/ddlBoJack/Omni-Captioner</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.12851v1" style="color: #0066cc; text-decoration: none;"><strong>Adaptive vector steering: A training-free, layer-wise intervention for hallucination mitigation in large audio and multimodal models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Note: This preprint is a version of the paper subm...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.11330v1" style="color: #0066cc; text-decoration: none;"><strong>Diffusion-Link: Diffusion Probabilistic Model for Bridging the Audio-Text Modality Gap</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-13</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages. Submitted to IEEE ICASSP 2026</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.04934v1" style="color: #0066cc; text-decoration: none;"><strong>AURA Score: A Metric For Holistic Audio Question Answering Evaluation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-06</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.03117v1" style="color: #0066cc; text-decoration: none;"><strong>Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-03</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.24635v1" style="color: #0066cc; text-decoration: none;"><strong>When Audio Generators Become Good Listeners: Generative Features for Understanding Tasks</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2506.02863v2" style="color: #0066cc; text-decoration: none;"><strong>CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-26</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.01659v2" style="color: #0066cc; text-decoration: none;"><strong>From Contrast to Commonality: Audio Commonality Captioning for Enhanced Audio-Text Cross-modal Understanding in Multimodal LLMs</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.17765v1" style="color: #0066cc; text-decoration: none;"><strong>Qwen3-Omni Technical Report</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">https://github.com/QwenLM/Qwen3-Omni</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2502.15178v3" style="color: #0066cc; text-decoration: none;"><strong>Enhancing Speech Large Language Models with Prompt-Aware Mixture of Audio Encoders</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">16 pages,5 figures, 13 tables, to be published in ...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.15680v1" style="color: #0066cc; text-decoration: none;"><strong>Mamba-2 audio captioning: design space exploration and analysis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to the 2026 IEEE International Conferenc...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.14659v1" style="color: #0066cc; text-decoration: none;"><strong>Aligning Audio Captions with Human Preferences</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-18</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.12591v1" style="color: #0066cc; text-decoration: none;"><strong>MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted in The 26th International Conference on W...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.19514v1" style="color: #0066cc; text-decoration: none;"><strong>MQAD: A Large-Scale Question Answering Dataset for Training Music Large Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-27</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Speech language model</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2505.13541v2" style="color: #0066cc; text-decoration: none;"><strong>SPIRIT: Patching Speech Language Models against Jailbreak Attacks</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2503.12115v2" style="color: #0066cc; text-decoration: none;"><strong>Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-15</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted by IEEE Journal of Selected Topics in Sig...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.12116v1" style="color: #0066cc; text-decoration: none;"><strong>Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted to EMNLP 2025 (Main Conference)</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.07978v1" style="color: #0066cc; text-decoration: none;"><strong>VoiceAgentBench: Are Voice Assistants ready for agentic tasks?</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-09</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.00981v2" style="color: #0066cc; text-decoration: none;"><strong>FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-02</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2510.01157v1" style="color: #0066cc; text-decoration: none;"><strong>Backdoor Attacks Against Speech Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-10-01</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.26276v1" style="color: #0066cc; text-decoration: none;"><strong>Optimizing Speech Language Models for Acoustic Consistency</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-30</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.11425v2" style="color: #0066cc; text-decoration: none;"><strong>FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2410.15017v2" style="color: #0066cc; text-decoration: none;"><strong>DM-Codec: Distilling Multimodal Representations for Speech Tokenization</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-29</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted at EMNLP 2025</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.21144v1" style="color: #0066cc; text-decoration: none;"><strong>UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.15362v2" style="color: #0066cc; text-decoration: none;"><strong>Speech Language Models for Under-Represented Languages: Insights from Wolof</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.18570v1" style="color: #0066cc; text-decoration: none;"><strong>HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages; submitted to ICASSP 2026</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.15655v1" style="color: #0066cc; text-decoration: none;"><strong>Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">EMNLP 2025 Main Conference (Oral)</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.14882v1" style="color: #0066cc; text-decoration: none;"><strong>Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-18</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 1 figures</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.13785v1" style="color: #0066cc; text-decoration: none;"><strong>Summary on The Multilingual Conversational Speech Language Model Challenge: Datasets, Tasks, Baselines, and Methods</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-17</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr></table>
    </div>

    <div style="
        margin-top: 40px;
        padding: 20px;
        background-color: #e9ecef;
        border-radius: 8px;
        text-align: center;
        color: #6c757d;
        font-size: 14px;
    ">
        <p style="margin: 0;">
            æœ¬é‚®ä»¶ç”± <strong>GitHub Actions</strong> è‡ªåŠ¨ç”Ÿæˆ<br>
            å¦‚æœ‰é—®é¢˜ï¼Œè¯·è®¿é—®é¡¹ç›®ä»“åº“æˆ–è”ç³»ç®¡ç†å‘˜
        </p>
    </div>
</body>
</html>
        