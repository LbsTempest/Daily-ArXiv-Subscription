
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>每日ArXiv论文更新</title>
</head>
<body style="
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    line-height: 1.6;
    color: #333;
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
    background-color: #ffffff;
">
    <div style="
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 30px;
        border-radius: 10px;
        text-align: center;
        margin-bottom: 30px;
    ">
        <h1 style="margin: 0; font-size: 28px;">📧 每日ArXiv论文更新</h1>
        <p style="margin: 10px 0 0 0; font-size: 16px; opacity: 0.9;">
            最新的学术论文，直达您的邮箱
        </p>
    </div>
    
    <div style="
        background-color: #f8f9fa;
        padding: 20px;
        border-radius: 8px;
        margin-bottom: 25px;
        border-left: 4px solid #007bff;
    ">
        <p style="margin: 0; color: #495057;">
            <strong>💡 提示：</strong>
            请查看 <a href="https://github.com/LbsTempest/Daily-ArXiv-Subscription" style="color: #007bff; text-decoration: none;">Github页面</a> 
            获得更好的阅读体验和更多论文。
        </p>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Speech Synthesis</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.19883v1" style="color: #0066cc; text-decoration: none;"><strong>CoMelSinger: Discrete Token-Based Zero-Shot Singing Synthesis With Structured Melody Control and Guidance</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-24</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">13 pages, 5 figures, 5 tables</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.19812v1" style="color: #0066cc; text-decoration: none;"><strong>Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-24</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">6 pages of main text, 1 page of references, 2 figu...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.19668v1" style="color: #0066cc; text-decoration: none;"><strong>Selective Classifier-free Guidance for Zero-shot Text-to-speech</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-24</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 7 figures, 1 table. Submitted to ICASSP 2...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2503.23108v3" style="color: #0066cc; text-decoration: none;"><strong>SupertonicTTS: Towards Highly Efficient and Streamlined Text-to-Speech System</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">22 pages, preprint</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.19001v1" style="color: #0066cc; text-decoration: none;"><strong>HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for Instruction-based TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 2 figures, submitted to ICASSP2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2501.04561v6" style="color: #0066cc; text-decoration: none;"><strong>OpenOmni: Advancing Open-Source Omnimodal Large Language Models with Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.18470v1" style="color: #0066cc; text-decoration: none;"><strong>Discrete-time diffusion-like models for speech synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.18060v1" style="color: #0066cc; text-decoration: none;"><strong>TMD-TTS: A Unified Tibetan Multi-Dialect Text-to-Speech Synthesis for Ü-Tsang, Amdo and Kham Speech Dataset Generation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.17052v1" style="color: #0066cc; text-decoration: none;"><strong>Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-21</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 1 figures</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.14579v2" style="color: #0066cc; text-decoration: none;"><strong>Cross-Lingual F5-TTS: Towards Language-Agnostic Voice Cloning and Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-20</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 2 figures</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.15492v1" style="color: #0066cc; text-decoration: none;"><strong>Beyond Video-to-SFX: Video to Audio Synthesis with Environmentally Aware Speech</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.14684v1" style="color: #0066cc; text-decoration: none;"><strong>DAIEN-TTS: Disentangled Audio Infilling for Environment-Aware Text-to-Speech Synthesis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-18</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2026</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2408.17432v3" style="color: #0066cc; text-decoration: none;"><strong>Text-to-Speech for Unseen Speakers via Low-Complexity Discrete Unit-Based Frame Selection</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-17</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Under review for IEEE OJSP</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2412.16846v2" style="color: #0066cc; text-decoration: none;"><strong>KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-17</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">6 figures, 5 tables</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.11425v1" style="color: #0066cc; text-decoration: none;"><strong>FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">TTS</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.19852v1" style="color: #0066cc; text-decoration: none;"><strong>Eliminating stability hallucinations in llm-based tts models via attention guidance</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-24</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, submitted to ICASSP2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.19001v1" style="color: #0066cc; text-decoration: none;"><strong>HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for Instruction-based TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 2 figures, submitted to ICASSP2026</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.18569v1" style="color: #0066cc; text-decoration: none;"><strong>Explore the Reinforcement Learning for the LLM based ASR and TTS system</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.18531v1" style="color: #0066cc; text-decoration: none;"><strong>No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">submitted to ICASSP 2026</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.17021v1" style="color: #0066cc; text-decoration: none;"><strong>Bridging the gap between training and inference in LM-based TTS models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-21</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 4 figures</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.09748v1" style="color: #0066cc; text-decoration: none;"><strong>DiTReducio: A Training-Free Acceleration for DiT-Based TTS via Progressive Calibration</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.05863v1" style="color: #0066cc; text-decoration: none;"><strong>LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-06</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.15442v3" style="color: #0066cc; text-decoration: none;"><strong>Mitigating Hallucinations in LM-Based TTS Models via Distribution Alignment Using GFlowNets</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-05</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted to EMNLP 2025 Main Conference (Oral)</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2506.23367v2" style="color: #0066cc; text-decoration: none;"><strong>You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-03</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted to ISCA Speech Synthesis Workshop, 2025, ...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.20513v1" style="color: #0066cc; text-decoration: none;"><strong>MoTAS: MoE-Guided Feature Selection from TTS-Augmented Speech for Enhanced Multimodal Alzheimer's Early Screening</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-28</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.14313v2" style="color: #0066cc; text-decoration: none;"><strong>Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2501.19258v2" style="color: #0066cc; text-decoration: none;"><strong>VisualSpeech: Enhancing Prosody Modeling in TTS Using Video</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.11326v1" style="color: #0066cc; text-decoration: none;"><strong>MoE-TTS: Enhancing Out-of-Domain Text Understanding for Description-based TTS via Mixture-of-Experts</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-15</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.05102v3" style="color: #0066cc; text-decoration: none;"><strong>Fairness in Dysarthric Speech Synthesis: Understanding Intrinsic Bias in Dysarthric Speech Cloning using F5-TTS</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-15</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted at Interspeech 2025</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.10412v1" style="color: #0066cc; text-decoration: none;"><strong>Facilitating Personalized TTS for Dysarthric Speakers Using Knowledge Anchoring and Curriculum Learning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Interspeech 2025</span></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Audio Caption</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.01659v2" style="color: #0066cc; text-decoration: none;"><strong>From Contrast to Commonality: Audio Commonality Captioning for Enhanced Audio-Text Cross-modal Understanding in Multimodal LLMs</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.17765v1" style="color: #0066cc; text-decoration: none;"><strong>Qwen3-Omni Technical Report</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">https://github.com/QwenLM/Qwen3-Omni</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2502.15178v3" style="color: #0066cc; text-decoration: none;"><strong>Enhancing Speech Large Language Models with Prompt-Aware Mixture of Audio Encoders</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">16 pages,5 figures, 13 tables, to be published in ...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.15680v1" style="color: #0066cc; text-decoration: none;"><strong>Mamba-2 audio captioning: design space exploration and analysis</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to the 2026 IEEE International Conferenc...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.14659v1" style="color: #0066cc; text-decoration: none;"><strong>Aligning Audio Captions with Human Preferences</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-18</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Submitted to ICASSP 2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.12591v1" style="color: #0066cc; text-decoration: none;"><strong>MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-16</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted in The 26th International Conference on W...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.19514v1" style="color: #0066cc; text-decoration: none;"><strong>MQAD: A Large-Scale Question Answering Dataset for Training Music Large Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-27</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2406.05914v3" style="color: #0066cc; text-decoration: none;"><strong>Soundscape Captioning using Sound Affective Quality Network and Large Language Model</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">IEEE Transactions on Multimedia, Code:
  https://g...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2507.18452v2" style="color: #0066cc; text-decoration: none;"><strong>DIFFA: Large Language Diffusion Models Can Listen and Understand</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-21</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2409.12962v2" style="color: #0066cc; text-decoration: none;"><strong>CLAIR-A: Leveraging Large Language Models to Judge Audio Captions</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted to ASRU 2025; Code is publicly available ...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.07829v1" style="color: #0066cc; text-decoration: none;"><strong>Auditory Intelligence: Understanding the World Through Sound</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-11</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Position paper without experimental/quantitative v...</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.03983v1" style="color: #0066cc; text-decoration: none;"><strong>MiDashengLM: Efficient Audio Understanding with General Audio Captions</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-06</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2507.18750v1" style="color: #0066cc; text-decoration: none;"><strong>CatchPhrase: EXPrompt-Guided Encoder Adaptation for Audio-to-Image Generation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-07-24</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2507.10419v1" style="color: #0066cc; text-decoration: none;"><strong>Multiple Choice Learning of Low Rank Adapters for Language Modeling</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-07-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2506.15220v2" style="color: #0066cc; text-decoration: none;"><strong>video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-07-10</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr></table>
    </div>

    <div style="margin-bottom: 35px;">
        <h2 style="
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
            font-size: 22px;
        ">Speech language model</h2>
        
        <table style="
            width: 100%; 
            border-collapse: collapse; 
            margin: 10px 0;
            font-family: Arial, sans-serif;
        ">
        <tr style="background-color: #f5f5f5;"><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Title</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Link</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Date</th><th style="padding: 12px 8px; text-align: left; border: 1px solid #ddd; font-weight: bold; color: #333;">Comment</th></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.18570v1" style="color: #0066cc; text-decoration: none;"><strong>HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-23</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages; submitted to ICASSP 2026</span></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.15655v1" style="color: #0066cc; text-decoration: none;"><strong>Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-19</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">EMNLP 2025 Main Conference (Oral)</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.15362v1" style="color: #0066cc; text-decoration: none;"><strong>Speech Language Models for Under-Represented Languages: Insights from Wolof</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-18</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.14882v1" style="color: #0066cc; text-decoration: none;"><strong>Llama-Mimi: Speech Language Models with Interleaved Semantic and Acoustic Tokens</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-18</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">5 pages, 1 figures</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.13785v1" style="color: #0066cc; text-decoration: none;"><strong>Summary on The Multilingual Conversational Speech Language Model Challenge: Datasets, Tasks, Baselines, and Methods</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-17</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.11425v1" style="color: #0066cc; text-decoration: none;"><strong>FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-14</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.04685v1" style="color: #0066cc; text-decoration: none;"><strong>Say More with Less: Variable-Frame-Rate Speech Tokenization via Adaptive Clustering and Implicit Duration Coding</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-04</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.05359v1" style="color: #0066cc; text-decoration: none;"><strong>An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-03</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Published in International Conference on Text, Spe...</span></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.09600v2" style="color: #0066cc; text-decoration: none;"><strong>OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via Understanding-Driven Spoken Dialogue</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-09-03</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2509.00503v1" style="color: #0066cc; text-decoration: none;"><strong>Entropy-based Coarse and Compressed Semantic Speech Representation Learning</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-30</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.20660v1" style="color: #0066cc; text-decoration: none;"><strong>CodecBench: A Comprehensive Benchmark for Acoustic and Semantic Evaluation</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-28</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2505.01263v2" style="color: #0066cc; text-decoration: none;"><strong>FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-25</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.16790v1" style="color: #0066cc; text-decoration: none;"><strong>TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-22</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #ffffff;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.15418v1" style="color: #0066cc; text-decoration: none;"><strong>LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-21</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td></tr><tr style="background-color: #f9f9f9;"><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><a href="http://arxiv.org/abs/2508.11224v1" style="color: #0066cc; text-decoration: none;"><strong>Benchmarking Prosody Encoding in Discrete Speech Tokens</strong></a></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"></td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;">2025-08-15</td><td style="padding: 10px 8px; border: 1px solid #ddd; vertical-align: top;"><span style="color: #666; font-size: 0.9em;">Accepted by ASRU2025</span></td></tr></table>
    </div>

    <div style="
        margin-top: 40px;
        padding: 20px;
        background-color: #e9ecef;
        border-radius: 8px;
        text-align: center;
        color: #6c757d;
        font-size: 14px;
    ">
        <p style="margin: 0;">
            本邮件由 <strong>GitHub Actions</strong> 自动生成<br>
            如有问题，请访问项目仓库或联系管理员
        </p>
    </div>
</body>
</html>
        