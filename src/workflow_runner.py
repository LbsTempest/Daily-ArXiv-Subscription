import logging
import time
from datetime import datetime
from typing import List, Dict

import pytz
import yaml

from .paper import Paper
from .arxiv_client import ArXivClient
from .markdown_generator import MarkdownGenerator

# è®¾ç½®åŸºç¡€æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class WorkflowRunner:
    def __init__(self, config_path: str):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.arxiv_client = ArXivClient(
            retries=self.config['arxiv']['retries'],
            wait_time_minutes=self.config['arxiv']['wait_time_minutes']
        )
        self.md_generator = MarkdownGenerator()

    def _process_entries(self, entries: List[Dict], target_fields: List[str]) -> List[Paper]:
        papers = []
        for entry in entries:
            tags = [tag['term'] for tag in entry.get('tags', [])]
            
            # è¿‡æ»¤é¢†åŸŸ
            if target_fields:
                if not any(tag.split('.')[0] in target_fields for tag in tags):
                    continue

            paper = Paper(
                title=" ".join(entry.title.replace("\n", " ").split()),
                link=entry.link,
                abstract=" ".join(entry.summary.replace("\n", " ").split()),
                authors=[author['name'] for author in entry.authors],
                tags=tags,
                comment=entry.get("arxiv_comment", ""),
                date=entry.updated
            )
            papers.append(paper)
        return papers

    def run(self):
        logging.info("å¼€å§‹æ‰§è¡Œæ¯æ—¥è®ºæ–‡æ›´æ–°å·¥ä½œæµ...")
        
        beijing_timezone = pytz.timezone('Asia/Shanghai')
        current_date = datetime.now(beijing_timezone).strftime("%Y-%m-%d")
        
        readme_content = [
            "# Daily Papers",
            "The project automatically fetches the latest papers from arXiv based on keywords.",
            "The subheadings in the README file represent the search keywords.",
            f"Last update: {current_date}\n"
        ]
        
        issue_title = f"Latest Papers - {datetime.now(beijing_timezone).strftime('%B %d, %Y')}"
        issue_content = [
            "---",
            f"title: {issue_title}",
            "labels: documentation",
            "---",
            "**Please check the [Github page](https://github.com/zezhishao/MTS_Daily_ArXiv) for a better reading experience and more papers.**\n"
        ]
        
        # ä¸ºé‚®ä»¶åˆ›å»ºå†…å®¹ï¼ˆä¸åŒ…å«YAML front matterï¼‰
        mail_content = [
            "# ğŸ“§ æ¯æ—¥ArXivè®ºæ–‡æ›´æ–°\n",
            "**è¯·æŸ¥çœ‹ [Githubé¡µé¢](https://github.com/zezhishao/MTS_Daily_ArXiv) è·å¾—æ›´å¥½çš„é˜…è¯»ä½“éªŒå’Œæ›´å¤šè®ºæ–‡ã€‚**\n"
        ]

        try:
            for keyword in self.config['keywords']:
                readme_content.append(f"## {keyword}")
                issue_content.append(f"## {keyword}")
                mail_content.append(f"## {keyword}")
                
                raw_entries = self.arxiv_client.fetch_papers(
                    keyword,
                    self.config['arxiv']['max_results_per_keyword']
                )

                if not raw_entries:
                    readme_content.append("Failed to fetch papers for this keyword.")
                    issue_content.append("Failed to fetch papers for this keyword.")
                    mail_content.append("Failed to fetch papers for this keyword.")
                    continue
                
                papers = self._process_entries(raw_entries, self.config['arxiv']['target_fields'])
                
                # ä¸º README ç”Ÿæˆè¡¨æ ¼
                readme_table = self.md_generator.generate_table(papers, self.config['output']['columns'])
                readme_content.append(readme_table + "\n")
                
                # ä¸º Issue ç”Ÿæˆè¡¨æ ¼
                issue_papers = papers[:self.config['output']['issue_max_papers']]
                issue_table = self.md_generator.generate_table(
                    issue_papers, 
                    self.config['output']['columns'],
                    self.config['output']['issue_ignore_columns']
                )
                issue_content.append(issue_table + "\n")
                
                # ä¸ºé‚®ä»¶ç”Ÿæˆè¡¨æ ¼ï¼ˆé™åˆ¶è®ºæ–‡æ•°é‡ï¼‰
                mail_papers = papers[:self.config['output']['issue_max_papers']]
                mail_table = self.md_generator.generate_table(
                    mail_papers,
                    self.config['output']['columns'],
                    self.config['output']['issue_ignore_columns']
                )
                mail_content.append(mail_table + "\n")
                
                time.sleep(self.config['arxiv']['request_delay_seconds'])

            # å¦‚æœæ‰€æœ‰æ­¥éª¤éƒ½æˆåŠŸï¼Œæ‰å†™å…¥æ–‡ä»¶ (åŸå­æ“ä½œ)
            with open(self.config['output']['readme_path'], 'w', encoding='utf-8') as f:
                f.write("\n".join(readme_content))
            logging.info(f"æˆåŠŸæ›´æ–° {self.config['output']['readme_path']}")

            with open(self.config['output']['issue_template_path'], 'w', encoding='utf-8') as f:
                f.write("\n".join(issue_content))
            logging.info(f"æˆåŠŸæ›´æ–° {self.config['output']['issue_template_path']}")
            
            # æ·»åŠ é‚®ä»¶æ¨¡æ¿åº•éƒ¨ä¿¡æ¯å¹¶å†™å…¥æ–‡ä»¶
            mail_content.append("\n---\n*æœ¬é‚®ä»¶ç”± GitHub Actions è‡ªåŠ¨ç”Ÿæˆ*")
            with open(self.config['output']['mail_template_path'], 'w', encoding='utf-8') as f:
                f.write("\n".join(mail_content))
            logging.info(f"æˆåŠŸæ›´æ–° {self.config['output']['mail_template_path']}")

        except Exception as e:
            logging.critical(f"å·¥ä½œæµæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
            # å¼‚å¸¸å‘ç”Ÿæ—¶ï¼Œä¸ä¼šå†™å…¥ä»»ä½•æ–‡ä»¶ï¼Œä»è€Œä¿æŒäº†åŸæœ‰æ–‡ä»¶çš„å®Œæ•´æ€§ï¼Œæ— éœ€å¤‡ä»½æ¢å¤
            
        logging.info("å·¥ä½œæµæ‰§è¡Œå®Œæ¯•ã€‚")